{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPceiCsH0hu78ooQhL6nNHv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["###**Student & UHI Submission Details**\n","\n","**Name:** Alanna Zimbehl  \n","**Student ID:** 19016467  \n","**Module:** Data Analytics on the Web  \n","**Assignment:** Assignment Two - Data Science (Python)\n","\n","---"],"metadata":{"id":"V_1kk-eGOYgh"}},{"cell_type":"markdown","source":["#Assignment Two - Introduction\n","Continuing on from the first section of the assignment, this notebook will contain the process and analysis of training various models, as well as analysing the accuracy of the predictions those models are making. \n","\n","#Linear Regression and Neural Networks\n","\n","**Neural Networks**\n","\n","A neural network is a deep learning technique which are comprised of multiple algorithms to detec patterns in data without having to actually manually coding an entire network. They're main idea was to mimick the process of a brain where each node acts a neuron, connected to many other neurons firing signals in this case passing data like a network, learning and repeating the process until it's completed. (IBM, 2023)\n","\n","**Linear Regressions**  \n","Linear Regression is another model that is used in machine learning, where the model will try predict the value of a variable based on the value of another variable. This is ideal to find relationships between variables and can otherwise be known as dependant and independant variables, where indepentant variables are used to predict the dependant variables. These particular models are ideal for making forecasting predictions or for spotting trends whether they are negative or positive. Typically there are visually depicted as scatter plots with various data points plotted and a calculation is perform to find the best line fit. This can help determine positive or negative corelatoins, these can also be weak where the data is quite scatter and has little meaning or strong where the data is relatively close together to signify importance. (IBM,2023)\n"],"metadata":{"id":"xGbbhx2LOjDw"}},{"cell_type":"markdown","source":["#Results\n","##Linear Regression Models"],"metadata":{"id":"-xQytnh5Ba-W"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hEoqBO6oOHW4","executionInfo":{"status":"ok","timestamp":1683305212372,"user_tz":-60,"elapsed":8408,"user":{"displayName":"Alanna Zimbehl","userId":"11428779735323257149"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5f0fd1bd-8b20-4f12-91c9-bfd0d015afda"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.12.0\n"]}],"source":["#import starting libraries\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","\n","#checking tensorflow\n","print(tf.__version__)\n"]},{"cell_type":"markdown","source":["To begin the training of the models, first need to import three libraries which will handle much of the process. To test the libraries have imported correctly, it's best to print out their installed versions which will aid in finding the correct documentation too."],"metadata":{"id":"V4qM7RzicqSo"}},{"cell_type":"code","source":["#loading the full dataset.\n","df_full = pd.read_csv('https://raw.githubusercontent.com/Behls/data-anayltics-uhi/main/assignmentTwo/data/collated_data_cleaned_final.csv')\n","print(df_full)\n","#loading the 2014 dataset.\n","df_2014 = pd.read_csv('https://raw.githubusercontent.com/Behls/data-anayltics-uhi/main/assignmentTwo/data/collated_data_year_2014.csv')\n","print(df_2014)"],"metadata":{"id":"Izn8_EDBnQWj","executionInfo":{"status":"ok","timestamp":1683315072227,"user_tz":-60,"elapsed":1466,"user":{"displayName":"Alanna Zimbehl","userId":"11428779735323257149"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b53357c3-5a81-4716-82b3-947534a06e50"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["      day  year  mo  da collision_date  temp  dewp     slp  visib  wdsp  \\\n","0       4  2020  12  31     31/12/2020  57.4  56.2  1024.4    2.0   5.6   \n","1       3  2020  12  30     30/12/2020  73.8  65.7  1015.8    8.3  10.1   \n","2       2  2020  12  29     29/12/2020  65.0  63.4  1013.5    6.6  12.0   \n","3       1  2020  12  28     28/12/2020  68.8  60.1  1013.0    9.4   7.6   \n","4       7  2020  12  27     27/12/2020  63.5  61.3  1011.9    2.3   5.0   \n","...   ...   ...  ..  ..            ...   ...   ...     ...    ...   ...   \n","3101    4  2012   7   5     05/07/2012  34.2  30.8  1011.1    6.4  11.9   \n","3102    3  2012   7   4     04/07/2012  36.5  25.7  1007.3   10.0  17.9   \n","3103    2  2012   7   3     03/07/2012  42.8  38.5  1019.4   10.0   9.8   \n","3104    1  2012   7   2     02/07/2012  40.4  24.6  1012.4   10.0  13.9   \n","3105    7  2012   7   1     01/07/2012  36.3  20.4  1018.1   10.0  12.6   \n","\n","      mxpsd  gust   max   min  prcp  sndp  fog  num_collisions  \n","0       8.9   0.0  69.1  53.1  0.00   0.0    1             278  \n","1      15.0  25.1  81.0  66.9  0.00   0.0    0             238  \n","2      17.1  27.0  68.0  62.6  0.05   0.0    0             244  \n","3      11.1  24.3  79.0  59.0  0.00   0.0    0             217  \n","4      11.1  24.3  75.9  59.0  0.00   0.0    1             184  \n","...     ...   ...   ...   ...   ...   ...  ...             ...  \n","3101   22.0  28.0  44.1  23.0  0.07   0.0    0             591  \n","3102   25.1  32.1  42.1  32.0  0.35   0.0    0             432  \n","3103   14.0  20.0  50.0  28.9  0.00   0.0    0             664  \n","3104   21.0  32.1  57.0  36.0  0.22   0.0    0             564  \n","3105   18.1  22.9  42.1  30.9  0.01   0.0    0             538  \n","\n","[3106 rows x 18 columns]\n","     day  year  mo  da collision_date  temp  dewp     slp  visib  wdsp  mxpsd  \\\n","0      3  2014  12  31     31/12/2014  69.2  64.7  1024.1    9.3   4.8    9.9   \n","1      2  2014  12  30     30/12/2014  68.3  55.6  1020.9   10.0   6.9   13.0   \n","2      1  2014  12  29     29/12/2014  55.8  42.5  1007.4    9.9   5.8   15.9   \n","3      7  2014  12  28     28/12/2014  45.4  43.2  1007.7    3.0  19.2   33.0   \n","4      6  2014  12  27     27/12/2014  64.8  59.0  1013.1    8.7   8.5   18.1   \n","..   ...   ...  ..  ..            ...   ...   ...     ...    ...   ...    ...   \n","360    7  2014   1   5     05/01/2014  70.0  64.4  1020.2    5.3   6.4   12.0   \n","361    6  2014   1   4     04/01/2014  74.3  68.6  1008.6    7.3  12.0   21.0   \n","362    5  2014   1   3     03/01/2014  33.4  27.2  1016.1    6.8  10.9   20.0   \n","363    4  2014   1   2     02/01/2014  62.7  58.6  1006.5    5.0   6.3   18.1   \n","364    3  2014   1   1     01/01/2014  69.5  61.4  1017.7   10.0   5.8   12.0   \n","\n","     gust   max   min  prcp  sndp  fog  num_collisions  \n","0    14.8  86.0  61.0  0.00   0.0    1             461  \n","1    14.8  81.0  60.1  0.00   0.0    0             469  \n","2    25.1  64.0  50.0  0.12   0.0    0             411  \n","3    45.1  52.0  33.1  0.41   0.0    1             349  \n","4    24.1  70.0  59.0  0.00   0.0    1             406  \n","..    ...   ...   ...   ...   ...  ...             ...  \n","360  20.9  77.0  62.1  0.07   0.0    1             320  \n","361  29.9  82.0  68.0  1.03   0.0    0             418  \n","362  28.0  36.0  30.2  0.36  15.0    0             423  \n","363  22.0  69.1  57.9  0.13   0.0    1             603  \n","364   0.0  78.1  61.0  0.00   0.0    0             399  \n","\n","[365 rows x 18 columns]\n"]}]},{"cell_type":"markdown","source":["Loading the full and seperated 2014 dataset from github, and then printing out the data to ensure all columns and rows are correct."],"metadata":{"id":"utANJ_wOLQqt"}},{"cell_type":"code","source":["#splitting each dataset - full dataset\n","inputs_full = [df_full['da'], df_full['mo'], df_full['temp'], df_full['prcp'], df_full['dewp'], df_full['num_collisions']]\n","headers_full = [\"day\", \"month\", \"temperature\", \"percipitation\",  \"dew point\", \"number collisions\"]\n","combined = pd.concat(inputs_full, axis=1, keys=headers_full)\n","print(combined)\n","\n","#splitting each dataset - 2014 dataset\n","inputs_2014 = [df_2014['da'],df_2014['mo'],df_2014['temp'],df_2014['max'],df_2014['min'],df_2014['prcp'], df_2014['wdsp'], df_2014['dewp'],df_2014['slp'], df_2014['num_collisions']]\n","headers_2014 = [\"day\", \"month\", \"temperature\",\"max temperature\", \"min temperature\", \"percipitation\", \"wind speed\", \"dew point\", \"sea-level pressure\", \"number collisions\"]\n","combined_2014 = pd.concat(inputs_2014, axis=1, keys=headers_2014)\n","print(combined_2014)\n","\n"],"metadata":{"id":"F4xQRw8lnLKD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683315075435,"user_tz":-60,"elapsed":416,"user":{"displayName":"Alanna Zimbehl","userId":"11428779735323257149"}},"outputId":"7f06c53c-2874-45ef-e977-921cb006fcc0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["      day  month  temperature  percipitation  dew point  number collisions\n","0      31     12         57.4           0.00       56.2                278\n","1      30     12         73.8           0.00       65.7                238\n","2      29     12         65.0           0.05       63.4                244\n","3      28     12         68.8           0.00       60.1                217\n","4      27     12         63.5           0.00       61.3                184\n","...   ...    ...          ...            ...        ...                ...\n","3101    5      7         34.2           0.07       30.8                591\n","3102    4      7         36.5           0.35       25.7                432\n","3103    3      7         42.8           0.00       38.5                664\n","3104    2      7         40.4           0.22       24.6                564\n","3105    1      7         36.3           0.01       20.4                538\n","\n","[3106 rows x 6 columns]\n","     day  month  temperature  max temperature  min temperature  percipitation  \\\n","0     31     12         69.2             86.0             61.0           0.00   \n","1     30     12         68.3             81.0             60.1           0.00   \n","2     29     12         55.8             64.0             50.0           0.12   \n","3     28     12         45.4             52.0             33.1           0.41   \n","4     27     12         64.8             70.0             59.0           0.00   \n","..   ...    ...          ...              ...              ...            ...   \n","360    5      1         70.0             77.0             62.1           0.07   \n","361    4      1         74.3             82.0             68.0           1.03   \n","362    3      1         33.4             36.0             30.2           0.36   \n","363    2      1         62.7             69.1             57.9           0.13   \n","364    1      1         69.5             78.1             61.0           0.00   \n","\n","     wind speed  dew point  sea-level pressure  number collisions  \n","0           4.8       64.7              1024.1                461  \n","1           6.9       55.6              1020.9                469  \n","2           5.8       42.5              1007.4                411  \n","3          19.2       43.2              1007.7                349  \n","4           8.5       59.0              1013.1                406  \n","..          ...        ...                 ...                ...  \n","360         6.4       64.4              1020.2                320  \n","361        12.0       68.6              1008.6                418  \n","362        10.9       27.2              1016.1                423  \n","363         6.3       58.6              1006.5                603  \n","364         5.8       61.4              1017.7                399  \n","\n","[365 rows x 10 columns]\n"]}]},{"cell_type":"markdown","source":["Then once the datasets are correctly imported, the features that will train and test the models will need to be saved into a new dataframe. This applies to both datasets. Additionally new headers would be applied to the dataset, which makes it a little easier to read where they will last be combined using pandas built in concat function."],"metadata":{"id":"1o8gc7NYLYU1"}},{"cell_type":"code","source":["#splitting train/test from full dataset\n","train_full = combined.sample(frac=0.80, random_state=0)\n","test_full = combined.drop(train_full.index)\n","\n","#splitting test/train from 2014 datset\n","train_2014 = combined_2014.sample(frac=0.80, random_state=0)\n","test_2014 = combined_2014.drop(train_2014.index)"],"metadata":{"id":"k-0THe7AZpf1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Each dataset will be further seperated into testing and training datasets, where there should be a total of four. Typically the training data is 80% and the testing data is 20% of the whole original dataset, however due to their small sizes these percentages may changed depending on the results from the models. Models can be overfitted, where they perform well during training but poorly during testing, so it would be benifical to allow these percentages to be flexible."],"metadata":{"id":"TZSFf_gdL0zD"}},{"cell_type":"code","source":["#print the test/train datasets\n","print(train_full)\n","print(test_full)\n","print(train_2014)\n","print(test_2014)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aQbLycXWckpN","executionInfo":{"status":"ok","timestamp":1683315654105,"user_tz":-60,"elapsed":511,"user":{"displayName":"Alanna Zimbehl","userId":"11428779735323257149"}},"outputId":"ebe2be78-3f59-4f16-d7a1-f906c48af5bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["      day  month  temperature  percipitation  dew point  number collisions\n","2554    3      1         33.4           0.36       27.2                423\n","253    22      4         31.1           0.00       11.6                148\n","1651   24      6         50.4           0.00       43.6                811\n","791     1     11         43.6           0.00       27.9                696\n","570    10      6         66.5           0.00       60.0                613\n","...   ...    ...          ...            ...        ...                ...\n","337    29      1         43.2           0.00       36.1                495\n","46     15     11         66.2           0.00       56.9                251\n","747    15     12         65.6           0.00       54.0                628\n","2836   27      3         35.4           0.00       25.4                441\n","753     9     12         73.5           0.00       59.6                465\n","\n","[2485 rows x 6 columns]\n","      day  month  temperature  percipitation  dew point  number collisions\n","0      31     12         57.4           0.00       56.2                278\n","3      28     12         68.8           0.00       60.1                217\n","7      24     12         71.1           0.00       69.7                315\n","21     10     12         42.2           0.00       36.4                311\n","24      7     12         32.5           0.00       26.2                281\n","...   ...    ...          ...            ...        ...                ...\n","3079   27      7         33.5           0.01       24.0                638\n","3083   23      7         53.4           0.08       50.1                505\n","3084   22      7         61.4           0.00       58.9                498\n","3087   19      7         43.8           0.00       28.3                522\n","3100    6      7         40.6           0.02       39.4                638\n","\n","[621 rows x 6 columns]\n","     day  month  temperature  max temperature  min temperature  percipitation  \\\n","106   16      9         53.6             61.0             50.0           0.05   \n","259   16      4         64.9             73.9             54.0           0.00   \n","45    16     11         71.1             77.0             68.0           0.01   \n","26     5     12         77.4             87.1             69.1           0.00   \n","78    14     10         69.4             73.9             64.9           0.01   \n","..   ...    ...          ...              ...              ...            ...   \n","227   18      5         55.2             64.0             46.0           0.00   \n","148    5      8         51.3             60.1             44.1           0.08   \n","143   10      8         59.0             66.2             53.1           0.00   \n","180    4      7         42.1             48.0             28.0           0.55   \n","131   22      8         55.4             59.0             52.0           0.10   \n","\n","     wind speed  dew point  sea-level pressure  number collisions  \n","106         9.9       50.4              1017.4                613  \n","259         4.7       59.5              1021.4                570  \n","45          6.8       69.4              1013.1                429  \n","26          7.3       68.4              1019.6                673  \n","78         10.0       66.9              1003.0                697  \n","..          ...        ...                 ...                ...  \n","227         7.3       50.0              1022.8                507  \n","148         6.4       50.5              1012.8                606  \n","143         3.3       40.4              1020.0                512  \n","180        12.3       38.5              1012.8                360  \n","131        12.9       52.9              1001.2                568  \n","\n","[292 rows x 10 columns]\n","     day  month  temperature  max temperature  min temperature  percipitation  \\\n","9     22     12         70.4             80.1             60.1           0.00   \n","25     6     12         70.8             78.1             66.9           0.00   \n","28     3     12         66.4             79.0             57.9           0.00   \n","31    30     11         67.9             75.0             62.1           0.00   \n","32    29     11         69.2             75.0             66.0           0.00   \n","..   ...    ...          ...              ...              ...            ...   \n","338   27      1         35.0             48.0             21.9           0.00   \n","350   15      1         44.0             57.0             39.9           0.00   \n","352   13      1         57.5             66.0             46.9           0.00   \n","360    5      1         70.0             77.0             62.1           0.07   \n","363    2      1         62.7             69.1             57.9           0.13   \n","\n","     wind speed  dew point  sea-level pressure  number collisions  \n","9           7.4       63.4              1020.2                566  \n","25          5.8       68.3              1011.1                628  \n","28          5.5       60.0              1016.2                569  \n","31          9.1       63.1              1009.0                377  \n","32          9.8       68.1              1025.8                394  \n","..          ...        ...                 ...                ...  \n","338         8.0       24.6              1030.7                511  \n","350         6.3       39.4              1013.2                583  \n","352         3.6       47.6              1022.0                568  \n","360         6.4       64.4              1020.2                320  \n","363         6.3       58.6              1006.5                603  \n","\n","[73 rows x 10 columns]\n"]}]},{"cell_type":"markdown","source":["To ensure the training and testing datasets have been correctly split, it's best to print and examine them for potential mistakes."],"metadata":{"id":"1PWXxPAtNobB"}},{"cell_type":"code","source":["#create copies of the datasets for training and testing - 2014 dataset\n","train_2014_copy = train_2014.copy()\n","test_2014_copy = test_2014.copy()\n","\n","#create copies of the datasets for training and testing - full dataset\n","train_full_copy = train_full.copy()\n","test_full_copy = test_full.copy()\n","\n","#creating labels - test\n","testing_labels_full = test_full_copy.pop('number collisions')\n","testing_labels_2014 = test_2014_copy.pop('number collisions')\n","\n","#creating labels - train\n","training_labels_full = train_full_copy.pop('number collisions')\n","training_labels_2014 = train_2014_copy.pop('number collisions')\n","\n","#testing labels work against dataset\n","print(train_2014_copy)\n","print(train_full_copy)\n","print(training_labels_2014)\n","\n","\n","\n","     "],"metadata":{"id":"H_JeCyrMcklp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683315689229,"user_tz":-60,"elapsed":415,"user":{"displayName":"Alanna Zimbehl","userId":"11428779735323257149"}},"outputId":"1b14fbff-67c4-43d9-b57b-7c2d9acbdc57"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["     day  month  temperature  max temperature  min temperature  percipitation  \\\n","106   16      9         53.6             61.0             50.0           0.05   \n","259   16      4         64.9             73.9             54.0           0.00   \n","45    16     11         71.1             77.0             68.0           0.01   \n","26     5     12         77.4             87.1             69.1           0.00   \n","78    14     10         69.4             73.9             64.9           0.01   \n","..   ...    ...          ...              ...              ...            ...   \n","227   18      5         55.2             64.0             46.0           0.00   \n","148    5      8         51.3             60.1             44.1           0.08   \n","143   10      8         59.0             66.2             53.1           0.00   \n","180    4      7         42.1             48.0             28.0           0.55   \n","131   22      8         55.4             59.0             52.0           0.10   \n","\n","     wind speed  dew point  sea-level pressure  \n","106         9.9       50.4              1017.4  \n","259         4.7       59.5              1021.4  \n","45          6.8       69.4              1013.1  \n","26          7.3       68.4              1019.6  \n","78         10.0       66.9              1003.0  \n","..          ...        ...                 ...  \n","227         7.3       50.0              1022.8  \n","148         6.4       50.5              1012.8  \n","143         3.3       40.4              1020.0  \n","180        12.3       38.5              1012.8  \n","131        12.9       52.9              1001.2  \n","\n","[292 rows x 9 columns]\n","      day  month  temperature  percipitation  dew point\n","2554    3      1         33.4           0.36       27.2\n","253    22      4         31.1           0.00       11.6\n","1651   24      6         50.4           0.00       43.6\n","791     1     11         43.6           0.00       27.9\n","570    10      6         66.5           0.00       60.0\n","...   ...    ...          ...            ...        ...\n","337    29      1         43.2           0.00       36.1\n","46     15     11         66.2           0.00       56.9\n","747    15     12         65.6           0.00       54.0\n","2836   27      3         35.4           0.00       25.4\n","753     9     12         73.5           0.00       59.6\n","\n","[2485 rows x 5 columns]\n","106    613\n","259    570\n","45     429\n","26     673\n","78     697\n","      ... \n","227    507\n","148    606\n","143    512\n","180    360\n","131    568\n","Name: number collisions, Length: 292, dtype: int64\n"]}]},{"cell_type":"markdown","source":["Finally this is the stage where the features are seperated from the labels otherwise known as outputs, so the model can train on features seperately and the ouputs to learn to predict outputs based on data passed to it. In this case the model is designed to predict the number of collisions based on various features, therefor the number of collisions column needs to be removed and stored in a label variable to used during training and testing. This will essentially split the dataset further, however it's also imported to make copies of the previous datasets so the originals remain untouched incase of errors."],"metadata":{"id":"8dffk-8RNq5V"}},{"cell_type":"code","source":["#linear regression - full\n","normalisation_layer = tf.keras.layers.Normalization(input_shape=[5,], axis=None)\n","normalisation_layer.adapt(np.array(train_full_copy))\n","\n","model_full = tf.keras.Sequential([\n"," normalisation_layer,\n"," tf.keras.layers.Dense(units=5)   \n","])\n","\n","model_full.compile(optimizer = tf.optimizers.Adam(learning_rate=0.2),\n","                   loss=\"mean_squared_error\"\n","                   )\n","\n","model_output = model_full.fit(train_full_copy, training_labels_full, epochs=100, validation_split=0.25)\n"],"metadata":{"id":"VDh0CYICckiD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683311095677,"user_tz":-60,"elapsed":26731,"user":{"displayName":"Alanna Zimbehl","userId":"11428779735323257149"}},"outputId":"2308a45e-d7de-484b-abe2-1074d3a626f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","59/59 [==============================] - 1s 5ms/step - loss: 302077.9688 - val_loss: 264238.1875\n","Epoch 2/100\n","59/59 [==============================] - 0s 3ms/step - loss: 243514.8750 - val_loss: 212465.2812\n","Epoch 3/100\n","59/59 [==============================] - 0s 3ms/step - loss: 195602.4844 - val_loss: 170238.0000\n","Epoch 4/100\n","59/59 [==============================] - 0s 3ms/step - loss: 156728.9844 - val_loss: 136659.6719\n","Epoch 5/100\n","59/59 [==============================] - 0s 3ms/step - loss: 125936.3516 - val_loss: 110193.4141\n","Epoch 6/100\n","59/59 [==============================] - 0s 3ms/step - loss: 101803.4062 - val_loss: 89687.2656\n","Epoch 7/100\n","59/59 [==============================] - 0s 3ms/step - loss: 83292.0547 - val_loss: 74297.3828\n","Epoch 8/100\n","59/59 [==============================] - 0s 3ms/step - loss: 69433.7500 - val_loss: 62910.3203\n","Epoch 9/100\n","59/59 [==============================] - 0s 3ms/step - loss: 59148.5156 - val_loss: 54569.1172\n","Epoch 10/100\n","59/59 [==============================] - 0s 3ms/step - loss: 51683.5547 - val_loss: 48586.0078\n","Epoch 11/100\n","59/59 [==============================] - 0s 3ms/step - loss: 46286.5352 - val_loss: 44367.5898\n","Epoch 12/100\n","59/59 [==============================] - 0s 2ms/step - loss: 42500.9922 - val_loss: 41376.6211\n","Epoch 13/100\n","59/59 [==============================] - 0s 3ms/step - loss: 39834.6641 - val_loss: 39314.0117\n","Epoch 14/100\n","59/59 [==============================] - 0s 3ms/step - loss: 37871.8164 - val_loss: 37784.6523\n","Epoch 15/100\n","59/59 [==============================] - 0s 3ms/step - loss: 36423.7773 - val_loss: 36604.7188\n","Epoch 16/100\n","59/59 [==============================] - 0s 3ms/step - loss: 35279.3750 - val_loss: 35670.8477\n","Epoch 17/100\n","59/59 [==============================] - 0s 2ms/step - loss: 34334.5938 - val_loss: 34861.6016\n","Epoch 18/100\n","59/59 [==============================] - 0s 2ms/step - loss: 33509.8008 - val_loss: 34115.7344\n","Epoch 19/100\n","59/59 [==============================] - 0s 2ms/step - loss: 32777.5625 - val_loss: 33426.4844\n","Epoch 20/100\n","59/59 [==============================] - 0s 3ms/step - loss: 32089.0625 - val_loss: 32779.8086\n","Epoch 21/100\n","59/59 [==============================] - 0s 2ms/step - loss: 31410.5723 - val_loss: 32124.3145\n","Epoch 22/100\n","59/59 [==============================] - 0s 2ms/step - loss: 30774.2500 - val_loss: 31501.9961\n","Epoch 23/100\n","59/59 [==============================] - 0s 3ms/step - loss: 30154.0312 - val_loss: 30915.4434\n","Epoch 24/100\n","59/59 [==============================] - 0s 3ms/step - loss: 29556.7109 - val_loss: 30318.4336\n","Epoch 25/100\n","59/59 [==============================] - 0s 3ms/step - loss: 28969.9102 - val_loss: 29733.9062\n","Epoch 26/100\n","59/59 [==============================] - 0s 3ms/step - loss: 28418.8672 - val_loss: 29182.2578\n","Epoch 27/100\n","59/59 [==============================] - 0s 4ms/step - loss: 27888.4297 - val_loss: 28656.8223\n","Epoch 28/100\n","59/59 [==============================] - 0s 4ms/step - loss: 27371.6035 - val_loss: 28148.2832\n","Epoch 29/100\n","59/59 [==============================] - 0s 4ms/step - loss: 26871.0547 - val_loss: 27652.2188\n","Epoch 30/100\n","59/59 [==============================] - 0s 7ms/step - loss: 26405.6250 - val_loss: 27196.2793\n","Epoch 31/100\n","59/59 [==============================] - 1s 12ms/step - loss: 25960.0059 - val_loss: 26760.6680\n","Epoch 32/100\n","59/59 [==============================] - 1s 9ms/step - loss: 25522.6738 - val_loss: 26333.3809\n","Epoch 33/100\n","59/59 [==============================] - 1s 10ms/step - loss: 25118.5273 - val_loss: 25937.6777\n","Epoch 34/100\n","59/59 [==============================] - 0s 5ms/step - loss: 24728.9785 - val_loss: 25559.4961\n","Epoch 35/100\n","59/59 [==============================] - 0s 3ms/step - loss: 24376.8477 - val_loss: 25206.9160\n","Epoch 36/100\n","59/59 [==============================] - 0s 3ms/step - loss: 24046.8340 - val_loss: 24878.9375\n","Epoch 37/100\n","59/59 [==============================] - 0s 3ms/step - loss: 23722.4551 - val_loss: 24586.2109\n","Epoch 38/100\n","59/59 [==============================] - 0s 3ms/step - loss: 23427.9062 - val_loss: 24288.6953\n","Epoch 39/100\n","59/59 [==============================] - 0s 3ms/step - loss: 23150.5684 - val_loss: 24026.5430\n","Epoch 40/100\n","59/59 [==============================] - 0s 2ms/step - loss: 22892.0293 - val_loss: 23767.7227\n","Epoch 41/100\n","59/59 [==============================] - 0s 3ms/step - loss: 22649.8730 - val_loss: 23544.7363\n","Epoch 42/100\n","59/59 [==============================] - 0s 2ms/step - loss: 22422.8047 - val_loss: 23315.5078\n","Epoch 43/100\n","59/59 [==============================] - 0s 2ms/step - loss: 22210.3906 - val_loss: 23103.1836\n","Epoch 44/100\n","59/59 [==============================] - 0s 3ms/step - loss: 22019.7012 - val_loss: 22917.7500\n","Epoch 45/100\n","59/59 [==============================] - 0s 4ms/step - loss: 21841.7070 - val_loss: 22727.1484\n","Epoch 46/100\n","59/59 [==============================] - 0s 3ms/step - loss: 21673.0664 - val_loss: 22564.5039\n","Epoch 47/100\n","59/59 [==============================] - 0s 6ms/step - loss: 21518.1836 - val_loss: 22422.5566\n","Epoch 48/100\n","59/59 [==============================] - 1s 9ms/step - loss: 21380.1270 - val_loss: 22284.2891\n","Epoch 49/100\n","59/59 [==============================] - 0s 6ms/step - loss: 21254.8633 - val_loss: 22160.4121\n","Epoch 50/100\n","59/59 [==============================] - 0s 3ms/step - loss: 21136.0859 - val_loss: 22050.7695\n","Epoch 51/100\n","59/59 [==============================] - 0s 4ms/step - loss: 21033.7422 - val_loss: 21955.4629\n","Epoch 52/100\n","59/59 [==============================] - 0s 5ms/step - loss: 20936.6914 - val_loss: 21841.8691\n","Epoch 53/100\n","59/59 [==============================] - 0s 2ms/step - loss: 20847.0684 - val_loss: 21761.4629\n","Epoch 54/100\n","59/59 [==============================] - 0s 5ms/step - loss: 20778.2793 - val_loss: 21669.7539\n","Epoch 55/100\n","59/59 [==============================] - 0s 4ms/step - loss: 20696.8203 - val_loss: 21592.4746\n","Epoch 56/100\n","59/59 [==============================] - 0s 4ms/step - loss: 20628.5879 - val_loss: 21515.2559\n","Epoch 57/100\n","59/59 [==============================] - 0s 5ms/step - loss: 20581.2266 - val_loss: 21456.5215\n","Epoch 58/100\n","59/59 [==============================] - 0s 4ms/step - loss: 20518.1973 - val_loss: 21400.7969\n","Epoch 59/100\n","59/59 [==============================] - 0s 7ms/step - loss: 20462.6797 - val_loss: 21335.0156\n","Epoch 60/100\n","59/59 [==============================] - 0s 6ms/step - loss: 20415.6172 - val_loss: 21282.8887\n","Epoch 61/100\n","59/59 [==============================] - 0s 6ms/step - loss: 20385.6074 - val_loss: 21243.9668\n","Epoch 62/100\n","59/59 [==============================] - 0s 6ms/step - loss: 20348.1504 - val_loss: 21191.3730\n","Epoch 63/100\n","59/59 [==============================] - 0s 4ms/step - loss: 20302.8340 - val_loss: 21161.3535\n","Epoch 64/100\n","59/59 [==============================] - 0s 5ms/step - loss: 20270.4727 - val_loss: 21108.6504\n","Epoch 65/100\n","59/59 [==============================] - 0s 5ms/step - loss: 20236.8418 - val_loss: 21066.8770\n","Epoch 66/100\n","59/59 [==============================] - 0s 4ms/step - loss: 20205.4434 - val_loss: 21038.3242\n","Epoch 67/100\n","59/59 [==============================] - 0s 5ms/step - loss: 20192.2383 - val_loss: 21010.1777\n","Epoch 68/100\n","59/59 [==============================] - 0s 6ms/step - loss: 20166.6094 - val_loss: 20985.8496\n","Epoch 69/100\n","59/59 [==============================] - 0s 4ms/step - loss: 20141.8184 - val_loss: 20964.7480\n","Epoch 70/100\n","59/59 [==============================] - 0s 6ms/step - loss: 20116.8359 - val_loss: 20928.9082\n","Epoch 71/100\n","59/59 [==============================] - 0s 4ms/step - loss: 20092.6465 - val_loss: 20891.1934\n","Epoch 72/100\n","59/59 [==============================] - 0s 6ms/step - loss: 20076.9727 - val_loss: 20856.6016\n","Epoch 73/100\n","59/59 [==============================] - 0s 6ms/step - loss: 20052.3535 - val_loss: 20871.5508\n","Epoch 74/100\n","59/59 [==============================] - 0s 7ms/step - loss: 20033.4766 - val_loss: 20804.8262\n","Epoch 75/100\n","59/59 [==============================] - 0s 6ms/step - loss: 20019.1074 - val_loss: 20785.2402\n","Epoch 76/100\n","59/59 [==============================] - 0s 6ms/step - loss: 20013.4629 - val_loss: 20756.0781\n","Epoch 77/100\n","59/59 [==============================] - 0s 4ms/step - loss: 19996.9883 - val_loss: 20719.2793\n","Epoch 78/100\n","59/59 [==============================] - 0s 5ms/step - loss: 19972.1562 - val_loss: 20728.9258\n","Epoch 79/100\n","59/59 [==============================] - 0s 7ms/step - loss: 19960.7266 - val_loss: 20686.3809\n","Epoch 80/100\n","59/59 [==============================] - 0s 4ms/step - loss: 19947.5273 - val_loss: 20681.0156\n","Epoch 81/100\n","59/59 [==============================] - 0s 6ms/step - loss: 19938.5859 - val_loss: 20665.8066\n","Epoch 82/100\n","59/59 [==============================] - 0s 3ms/step - loss: 19919.6055 - val_loss: 20653.6621\n","Epoch 83/100\n","59/59 [==============================] - 0s 4ms/step - loss: 19902.1914 - val_loss: 20630.4316\n","Epoch 84/100\n","59/59 [==============================] - 0s 3ms/step - loss: 19891.4629 - val_loss: 20617.4941\n","Epoch 85/100\n","59/59 [==============================] - 0s 4ms/step - loss: 19886.1113 - val_loss: 20599.1934\n","Epoch 86/100\n","59/59 [==============================] - 0s 3ms/step - loss: 19859.9082 - val_loss: 20567.4688\n","Epoch 87/100\n","59/59 [==============================] - 0s 5ms/step - loss: 19854.7598 - val_loss: 20549.9980\n","Epoch 88/100\n","59/59 [==============================] - 0s 3ms/step - loss: 19837.9375 - val_loss: 20544.3047\n","Epoch 89/100\n","59/59 [==============================] - 0s 4ms/step - loss: 19824.0723 - val_loss: 20524.8145\n","Epoch 90/100\n","59/59 [==============================] - 0s 4ms/step - loss: 19814.1641 - val_loss: 20492.5547\n","Epoch 91/100\n","59/59 [==============================] - 1s 10ms/step - loss: 19815.0469 - val_loss: 20520.5137\n","Epoch 92/100\n","59/59 [==============================] - 0s 4ms/step - loss: 19789.2266 - val_loss: 20479.5332\n","Epoch 93/100\n","59/59 [==============================] - 0s 5ms/step - loss: 19772.3301 - val_loss: 20448.5723\n","Epoch 94/100\n","59/59 [==============================] - 0s 3ms/step - loss: 19763.4551 - val_loss: 20431.3105\n","Epoch 95/100\n","59/59 [==============================] - 0s 4ms/step - loss: 19750.3984 - val_loss: 20415.6836\n","Epoch 96/100\n","59/59 [==============================] - 0s 3ms/step - loss: 19737.2031 - val_loss: 20402.2539\n","Epoch 97/100\n","59/59 [==============================] - 0s 4ms/step - loss: 19724.3867 - val_loss: 20424.2773\n","Epoch 98/100\n","59/59 [==============================] - 0s 4ms/step - loss: 19712.7910 - val_loss: 20380.6816\n","Epoch 99/100\n","59/59 [==============================] - 0s 3ms/step - loss: 19714.8203 - val_loss: 20413.7012\n","Epoch 100/100\n","59/59 [==============================] - 0s 5ms/step - loss: 19709.2188 - val_loss: 20373.4727\n"]}]},{"cell_type":"markdown","source":["At this stage it's a case of setting up the configuration such as normalisation for the model. In this case as the full dataset contains five feature columns the normalisation process would ideally take five inputs at a time as it needs to convert five features to predict one output value. As this dataset is relatively small it wouldn't require much processing power or time to do at least 100 passes on each sample. The normalisation layer takes a range of values, finds the minimum and then the maxmimum assigns them values bet 0 and -1 and every value inbetween it calculates a normalised value between those figures. During the analysis stage there where two loss functions that where chosen to measure the errors within the dataset, first being mean squared error and the second being root mean squared error. \n","\n","The linear regression model appears to be preforming quite poorly, ideally would like to see lower loss numbers and these values are closely validated by the validation loss therefor it's likely this model's accuracy making predictions will be quite poor. In this particular model the mean squared error loss function was used, where it will find the mean squared errors between the actual real values and ones that are predicted. (Neptune AI, 2023) The epochs where increased from 10 to 100 so it makes more passes over the datset, and that didn't return any noticable improvements to the accuracy of the model. Originall the epochs where set to 1000 however it was evident the loss values didnt decrease enough for it to be viable, additionally the different between the loss value and validation value increase where it should infact decrease. Additionally the learning rate was increased which did improve the returned results slightly."],"metadata":{"id":"sNMhLzuUBUnr"}},{"cell_type":"code","source":["#new data\n","new_data = np.array([30,12,57,0.00,56])\n","predictions = model_full.predict(new_data)\n","print(predictions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hpIzR5DJ9RoY","executionInfo":{"status":"ok","timestamp":1683314727251,"user_tz":-60,"elapsed":714,"user":{"displayName":"Alanna Zimbehl","userId":"11428779735323257149"}},"outputId":"f9b867a4-5feb-45ab-d373-9f0c66e8d453"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 22ms/step\n","[[517.1939  517.31696 517.2752  517.24243 517.46106]]\n"]}]},{"cell_type":"markdown","source":["Once the model was trained a new dataset with similar values would be passed into the model to predict the number of collisions. According to the results it perform quite poorly. The dataframe consists of intial date of a day, which is the 30th, followed by the month 12 which is december, temperature and percipitation values of 56°F and 0.00, finally followed by the dew point for the day which was 56°F. It should predict a value between 278 and 244 as it is based on the similar values before and after in the dataset, however based on the results of 517 it has performed very poorly and isn't accurate."],"metadata":{"id":"A0qVe9NoKJyU"}},{"cell_type":"code","source":["#linear regression - 2014\n","normalisation_layer_2014 = tf.keras.layers.Normalization(input_shape=[9,], axis=None)\n","normalisation_layer_2014.adapt(np.array(train_2014_copy))\n","\n","model_2014 = tf.keras.Sequential([\n"," normalisation_layer_2014,\n"," tf.keras.layers.Dense(units=10)   \n","])\n","\n","model_2014.compile(optimizer = tf.optimizers.Adam(learning_rate=0.1),\n","                   loss=\"mean_squared_error\",\n","                   metrics = [tf.keras.metrics.RootMeanSquaredError(name='rmse')]\n","                   )\n","\n","model_output = model_2014.fit(train_2014_copy, training_labels_2014, epochs=100, validation_split=0.2, verbose=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7A6zhBNS4OwP","executionInfo":{"status":"ok","timestamp":1683315286644,"user_tz":-60,"elapsed":7644,"user":{"displayName":"Alanna Zimbehl","userId":"11428779735323257149"}},"outputId":"2a018a92-50a4-42b1-90ea-685d5542676d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","8/8 [==============================] - 1s 27ms/step - loss: 328058.2188 - rmse: 572.7636 - val_loss: 320477.4375 - val_rmse: 566.1072\n","Epoch 2/100\n","8/8 [==============================] - 0s 9ms/step - loss: 322100.7812 - rmse: 567.5392 - val_loss: 314573.9375 - val_rmse: 560.8689\n","Epoch 3/100\n","8/8 [==============================] - 0s 10ms/step - loss: 316201.4375 - rmse: 562.3179 - val_loss: 308767.3125 - val_rmse: 555.6683\n","Epoch 4/100\n","8/8 [==============================] - 0s 21ms/step - loss: 310405.7812 - rmse: 557.1407 - val_loss: 303015.1875 - val_rmse: 550.4682\n","Epoch 5/100\n","8/8 [==============================] - 0s 17ms/step - loss: 304657.7188 - rmse: 551.9580 - val_loss: 297342.3750 - val_rmse: 545.2911\n","Epoch 6/100\n","8/8 [==============================] - 0s 18ms/step - loss: 298982.2812 - rmse: 546.7927 - val_loss: 291759.2500 - val_rmse: 540.1475\n","Epoch 7/100\n","8/8 [==============================] - 0s 10ms/step - loss: 293395.2812 - rmse: 541.6598 - val_loss: 286234.4688 - val_rmse: 535.0089\n","Epoch 8/100\n","8/8 [==============================] - 0s 11ms/step - loss: 287895.1875 - rmse: 536.5587 - val_loss: 280789.2500 - val_rmse: 529.8955\n","Epoch 9/100\n","8/8 [==============================] - 0s 6ms/step - loss: 282482.3125 - rmse: 531.4906 - val_loss: 275423.6875 - val_rmse: 524.8082\n","Epoch 10/100\n","8/8 [==============================] - 0s 7ms/step - loss: 277111.2812 - rmse: 526.4136 - val_loss: 270162.8750 - val_rmse: 519.7720\n","Epoch 11/100\n","8/8 [==============================] - 0s 7ms/step - loss: 271862.1562 - rmse: 521.4040 - val_loss: 264978.6250 - val_rmse: 514.7607\n","Epoch 12/100\n","8/8 [==============================] - 0s 7ms/step - loss: 266676.9375 - rmse: 516.4077 - val_loss: 259869.8438 - val_rmse: 509.7743\n","Epoch 13/100\n","8/8 [==============================] - 0s 8ms/step - loss: 261586.5625 - rmse: 511.4554 - val_loss: 254831.1875 - val_rmse: 504.8081\n","Epoch 14/100\n","8/8 [==============================] - 0s 6ms/step - loss: 256512.8281 - rmse: 506.4709 - val_loss: 249879.5625 - val_rmse: 499.8795\n","Epoch 15/100\n","8/8 [==============================] - 0s 9ms/step - loss: 251601.5156 - rmse: 501.5990 - val_loss: 244939.1562 - val_rmse: 494.9133\n","Epoch 16/100\n","8/8 [==============================] - 0s 7ms/step - loss: 246660.5625 - rmse: 496.6493 - val_loss: 240125.3750 - val_rmse: 490.0259\n","Epoch 17/100\n","8/8 [==============================] - 0s 8ms/step - loss: 241844.4688 - rmse: 491.7768 - val_loss: 235389.7969 - val_rmse: 485.1699\n","Epoch 18/100\n","8/8 [==============================] - 0s 7ms/step - loss: 237116.4688 - rmse: 486.9460 - val_loss: 230700.6406 - val_rmse: 480.3131\n","Epoch 19/100\n","8/8 [==============================] - 0s 6ms/step - loss: 232471.0625 - rmse: 482.1525 - val_loss: 226084.1562 - val_rmse: 475.4831\n","Epoch 20/100\n","8/8 [==============================] - 0s 8ms/step - loss: 227836.3125 - rmse: 477.3220 - val_loss: 221549.3594 - val_rmse: 470.6903\n","Epoch 21/100\n","8/8 [==============================] - 0s 8ms/step - loss: 223308.7656 - rmse: 472.5556 - val_loss: 217113.5312 - val_rmse: 465.9544\n","Epoch 22/100\n","8/8 [==============================] - 0s 7ms/step - loss: 218873.6875 - rmse: 467.8394 - val_loss: 212727.7500 - val_rmse: 461.2242\n","Epoch 23/100\n","8/8 [==============================] - 0s 6ms/step - loss: 214541.3906 - rmse: 463.1861 - val_loss: 208397.6875 - val_rmse: 456.5060\n","Epoch 24/100\n","8/8 [==============================] - 0s 6ms/step - loss: 210205.7188 - rmse: 458.4820 - val_loss: 204186.5312 - val_rmse: 451.8700\n","Epoch 25/100\n","8/8 [==============================] - 0s 7ms/step - loss: 206017.8125 - rmse: 453.8919 - val_loss: 200023.5938 - val_rmse: 447.2399\n","Epoch 26/100\n","8/8 [==============================] - 0s 9ms/step - loss: 201883.7031 - rmse: 449.3147 - val_loss: 195926.9375 - val_rmse: 442.6364\n","Epoch 27/100\n","8/8 [==============================] - 0s 9ms/step - loss: 197774.1406 - rmse: 444.7181 - val_loss: 191918.1719 - val_rmse: 438.0847\n","Epoch 28/100\n","8/8 [==============================] - 0s 6ms/step - loss: 193771.0156 - rmse: 440.1943 - val_loss: 187920.8281 - val_rmse: 433.4984\n","Epoch 29/100\n","8/8 [==============================] - 0s 6ms/step - loss: 189790.7344 - rmse: 435.6497 - val_loss: 184004.7344 - val_rmse: 428.9577\n","Epoch 30/100\n","8/8 [==============================] - 0s 7ms/step - loss: 185890.8594 - rmse: 431.1507 - val_loss: 180170.5938 - val_rmse: 424.4651\n","Epoch 31/100\n","8/8 [==============================] - 0s 6ms/step - loss: 182072.5781 - rmse: 426.6996 - val_loss: 176405.9375 - val_rmse: 420.0071\n","Epoch 32/100\n","8/8 [==============================] - 0s 6ms/step - loss: 178296.7969 - rmse: 422.2520 - val_loss: 172721.7969 - val_rmse: 415.5981\n","Epoch 33/100\n","8/8 [==============================] - 0s 7ms/step - loss: 174637.1562 - rmse: 417.8961 - val_loss: 169081.7656 - val_rmse: 411.1955\n","Epoch 34/100\n","8/8 [==============================] - 0s 6ms/step - loss: 171026.9219 - rmse: 413.5540 - val_loss: 165477.7656 - val_rmse: 406.7896\n","Epoch 35/100\n","8/8 [==============================] - 0s 6ms/step - loss: 167424.8906 - rmse: 409.1758 - val_loss: 161968.9375 - val_rmse: 402.4537\n","Epoch 36/100\n","8/8 [==============================] - 0s 7ms/step - loss: 163924.4844 - rmse: 404.8759 - val_loss: 158504.6406 - val_rmse: 398.1264\n","Epoch 37/100\n","8/8 [==============================] - 0s 9ms/step - loss: 160463.9531 - rmse: 400.5795 - val_loss: 155065.9844 - val_rmse: 393.7842\n","Epoch 38/100\n","8/8 [==============================] - 0s 7ms/step - loss: 157022.2812 - rmse: 396.2604 - val_loss: 151674.9531 - val_rmse: 389.4547\n","Epoch 39/100\n","8/8 [==============================] - 0s 8ms/step - loss: 153672.1094 - rmse: 392.0103 - val_loss: 148366.4531 - val_rmse: 385.1836\n","Epoch 40/100\n","8/8 [==============================] - 0s 6ms/step - loss: 150358.5938 - rmse: 387.7610 - val_loss: 145140.0156 - val_rmse: 380.9725\n","Epoch 41/100\n","8/8 [==============================] - 0s 9ms/step - loss: 147133.3594 - rmse: 383.5797 - val_loss: 141942.4375 - val_rmse: 376.7525\n","Epoch 42/100\n","8/8 [==============================] - 0s 9ms/step - loss: 143964.6875 - rmse: 379.4268 - val_loss: 138802.5625 - val_rmse: 372.5622\n","Epoch 43/100\n","8/8 [==============================] - 0s 6ms/step - loss: 140830.6562 - rmse: 375.2741 - val_loss: 135701.7188 - val_rmse: 368.3771\n","Epoch 44/100\n","8/8 [==============================] - 0s 9ms/step - loss: 137762.1250 - rmse: 371.1632 - val_loss: 132694.0312 - val_rmse: 364.2719\n","Epoch 45/100\n","8/8 [==============================] - 0s 6ms/step - loss: 134776.7969 - rmse: 367.1196 - val_loss: 129732.3203 - val_rmse: 360.1837\n","Epoch 46/100\n","8/8 [==============================] - 0s 6ms/step - loss: 131845.8750 - rmse: 363.1059 - val_loss: 126829.7266 - val_rmse: 356.1316\n","Epoch 47/100\n","8/8 [==============================] - 0s 6ms/step - loss: 128981.2422 - rmse: 359.1396 - val_loss: 124003.6094 - val_rmse: 352.1414\n","Epoch 48/100\n","8/8 [==============================] - 0s 9ms/step - loss: 126150.0859 - rmse: 355.1761 - val_loss: 121247.1406 - val_rmse: 348.2056\n","Epoch 49/100\n","8/8 [==============================] - 0s 8ms/step - loss: 123404.5703 - rmse: 351.2899 - val_loss: 118535.1328 - val_rmse: 344.2893\n","Epoch 50/100\n","8/8 [==============================] - 0s 8ms/step - loss: 120715.0469 - rmse: 347.4407 - val_loss: 115866.0625 - val_rmse: 340.3911\n","Epoch 51/100\n","8/8 [==============================] - 0s 9ms/step - loss: 118038.2344 - rmse: 343.5669 - val_loss: 113263.8984 - val_rmse: 336.5470\n","Epoch 52/100\n","8/8 [==============================] - 0s 8ms/step - loss: 115458.7266 - rmse: 339.7922 - val_loss: 110689.6094 - val_rmse: 332.7005\n","Epoch 53/100\n","8/8 [==============================] - 0s 9ms/step - loss: 112903.1250 - rmse: 336.0106 - val_loss: 108155.4766 - val_rmse: 328.8700\n","Epoch 54/100\n","8/8 [==============================] - 0s 6ms/step - loss: 110373.6484 - rmse: 332.2253 - val_loss: 105672.2734 - val_rmse: 325.0727\n","Epoch 55/100\n","8/8 [==============================] - 0s 9ms/step - loss: 107905.9766 - rmse: 328.4904 - val_loss: 103227.1328 - val_rmse: 321.2898\n","Epoch 56/100\n","8/8 [==============================] - 0s 8ms/step - loss: 105482.6719 - rmse: 324.7809 - val_loss: 100841.5000 - val_rmse: 317.5555\n","Epoch 57/100\n","8/8 [==============================] - 0s 7ms/step - loss: 103131.8906 - rmse: 321.1415 - val_loss: 98478.1172 - val_rmse: 313.8122\n","Epoch 58/100\n","8/8 [==============================] - 0s 6ms/step - loss: 100773.0547 - rmse: 317.4477 - val_loss: 96198.5312 - val_rmse: 310.1589\n","Epoch 59/100\n","8/8 [==============================] - 0s 7ms/step - loss: 98514.1484 - rmse: 313.8696 - val_loss: 93937.8281 - val_rmse: 306.4928\n","Epoch 60/100\n","8/8 [==============================] - 0s 7ms/step - loss: 96259.3672 - rmse: 310.2569 - val_loss: 91740.8438 - val_rmse: 302.8875\n","Epoch 61/100\n","8/8 [==============================] - 0s 6ms/step - loss: 94073.1016 - rmse: 306.7134 - val_loss: 89574.8281 - val_rmse: 299.2905\n","Epoch 62/100\n","8/8 [==============================] - 0s 6ms/step - loss: 91933.1641 - rmse: 303.2048 - val_loss: 87447.0000 - val_rmse: 295.7144\n","Epoch 63/100\n","8/8 [==============================] - 0s 6ms/step - loss: 89824.9219 - rmse: 299.7081 - val_loss: 85392.5781 - val_rmse: 292.2201\n","Epoch 64/100\n","8/8 [==============================] - 0s 8ms/step - loss: 87788.7266 - rmse: 296.2916 - val_loss: 83378.3906 - val_rmse: 288.7531\n","Epoch 65/100\n","8/8 [==============================] - 0s 8ms/step - loss: 85766.0312 - rmse: 292.8584 - val_loss: 81402.4219 - val_rmse: 285.3111\n","Epoch 66/100\n","8/8 [==============================] - 0s 8ms/step - loss: 83819.7344 - rmse: 289.5164 - val_loss: 79439.2734 - val_rmse: 281.8497\n","Epoch 67/100\n","8/8 [==============================] - 0s 8ms/step - loss: 81861.2891 - rmse: 286.1141 - val_loss: 77525.1172 - val_rmse: 278.4333\n","Epoch 68/100\n","8/8 [==============================] - 0s 6ms/step - loss: 79963.2812 - rmse: 282.7778 - val_loss: 75657.9297 - val_rmse: 275.0598\n","Epoch 69/100\n","8/8 [==============================] - 0s 6ms/step - loss: 78110.8672 - rmse: 279.4832 - val_loss: 73838.2734 - val_rmse: 271.7320\n","Epoch 70/100\n","8/8 [==============================] - 0s 6ms/step - loss: 76302.3750 - rmse: 276.2289 - val_loss: 72056.0156 - val_rmse: 268.4325\n","Epoch 71/100\n","8/8 [==============================] - 0s 8ms/step - loss: 74549.8906 - rmse: 273.0383 - val_loss: 70299.9688 - val_rmse: 265.1414\n","Epoch 72/100\n","8/8 [==============================] - 0s 9ms/step - loss: 72810.1484 - rmse: 269.8336 - val_loss: 68589.0859 - val_rmse: 261.8951\n","Epoch 73/100\n","8/8 [==============================] - 0s 6ms/step - loss: 71097.8281 - rmse: 266.6418 - val_loss: 66935.2578 - val_rmse: 258.7185\n","Epoch 74/100\n","8/8 [==============================] - 0s 7ms/step - loss: 69460.4141 - rmse: 263.5534 - val_loss: 65286.4375 - val_rmse: 255.5121\n","Epoch 75/100\n","8/8 [==============================] - 0s 6ms/step - loss: 67813.4609 - rmse: 260.4102 - val_loss: 63655.1406 - val_rmse: 252.2997\n","Epoch 76/100\n","8/8 [==============================] - 0s 8ms/step - loss: 66201.3203 - rmse: 257.2962 - val_loss: 62092.4727 - val_rmse: 249.1836\n","Epoch 77/100\n","8/8 [==============================] - 0s 10ms/step - loss: 64679.9844 - rmse: 254.3226 - val_loss: 60541.4492 - val_rmse: 246.0517\n","Epoch 78/100\n","8/8 [==============================] - 0s 8ms/step - loss: 63138.8672 - rmse: 251.2745 - val_loss: 59043.0039 - val_rmse: 242.9877\n","Epoch 79/100\n","8/8 [==============================] - 0s 6ms/step - loss: 61643.0469 - rmse: 248.2802 - val_loss: 57586.3203 - val_rmse: 239.9715\n","Epoch 80/100\n","8/8 [==============================] - 0s 7ms/step - loss: 60194.6445 - rmse: 245.3460 - val_loss: 56154.4141 - val_rmse: 236.9692\n","Epoch 81/100\n","8/8 [==============================] - 0s 6ms/step - loss: 58792.0625 - rmse: 242.4707 - val_loss: 54759.3164 - val_rmse: 234.0071\n","Epoch 82/100\n","8/8 [==============================] - 0s 9ms/step - loss: 57416.6992 - rmse: 239.6178 - val_loss: 53384.9414 - val_rmse: 231.0518\n","Epoch 83/100\n","8/8 [==============================] - 0s 9ms/step - loss: 56045.9922 - rmse: 236.7404 - val_loss: 52040.9336 - val_rmse: 228.1248\n","Epoch 84/100\n","8/8 [==============================] - 0s 6ms/step - loss: 54709.9180 - rmse: 233.9015 - val_loss: 50723.9570 - val_rmse: 225.2198\n","Epoch 85/100\n","8/8 [==============================] - 0s 8ms/step - loss: 53420.6523 - rmse: 231.1291 - val_loss: 49424.8047 - val_rmse: 222.3169\n","Epoch 86/100\n","8/8 [==============================] - 0s 9ms/step - loss: 52125.5117 - rmse: 228.3101 - val_loss: 48183.2227 - val_rmse: 219.5068\n","Epoch 87/100\n","8/8 [==============================] - 0s 6ms/step - loss: 50910.1328 - rmse: 225.6328 - val_loss: 46969.7656 - val_rmse: 216.7251\n","Epoch 88/100\n","8/8 [==============================] - 0s 9ms/step - loss: 49703.3555 - rmse: 222.9425 - val_loss: 45784.4062 - val_rmse: 213.9729\n","Epoch 89/100\n","8/8 [==============================] - 0s 6ms/step - loss: 48541.4805 - rmse: 220.3213 - val_loss: 44611.8906 - val_rmse: 211.2153\n","Epoch 90/100\n","8/8 [==============================] - 0s 6ms/step - loss: 47383.4297 - rmse: 217.6774 - val_loss: 43478.1445 - val_rmse: 208.5141\n","Epoch 91/100\n","8/8 [==============================] - 0s 6ms/step - loss: 46268.7461 - rmse: 215.1017 - val_loss: 42387.7070 - val_rmse: 205.8828\n","Epoch 92/100\n","8/8 [==============================] - 0s 7ms/step - loss: 45188.5078 - rmse: 212.5759 - val_loss: 41329.6953 - val_rmse: 203.2971\n","Epoch 93/100\n","8/8 [==============================] - 0s 8ms/step - loss: 44150.4766 - rmse: 210.1201 - val_loss: 40271.1289 - val_rmse: 200.6767\n","Epoch 94/100\n","8/8 [==============================] - 0s 10ms/step - loss: 43113.3164 - rmse: 207.6375 - val_loss: 39263.1875 - val_rmse: 198.1494\n","Epoch 95/100\n","8/8 [==============================] - 0s 6ms/step - loss: 42120.5391 - rmse: 205.2329 - val_loss: 38276.5664 - val_rmse: 195.6440\n","Epoch 96/100\n","8/8 [==============================] - 0s 6ms/step - loss: 41132.5312 - rmse: 202.8116 - val_loss: 37327.8789 - val_rmse: 193.2042\n","Epoch 97/100\n","8/8 [==============================] - 0s 8ms/step - loss: 40200.2305 - rmse: 200.5000 - val_loss: 36396.5273 - val_rmse: 190.7787\n","Epoch 98/100\n","8/8 [==============================] - 0s 9ms/step - loss: 39273.3164 - rmse: 198.1750 - val_loss: 35489.5938 - val_rmse: 188.3868\n","Epoch 99/100\n","8/8 [==============================] - 0s 8ms/step - loss: 38388.4375 - rmse: 195.9297 - val_loss: 34604.7852 - val_rmse: 186.0236\n","Epoch 100/100\n","8/8 [==============================] - 0s 8ms/step - loss: 37521.9414 - rmse: 193.7058 - val_loss: 33743.6953 - val_rmse: 183.6946\n"]}]},{"cell_type":"markdown","source":["Much like the previous linear regression, this one is training on a specific dataset which year is 2014. There is far less data, therefore far less for the model to train on. This dataset has nine columns, therefor the input shape is different to reflect that, furthermore the model is utilising the metric option moot mean square error. Both loss functions penalise larger errors however the difference mainly between the two is that the root square mean error function is measured in similar form to the predicted value. According to the results, there are quite large descrepencies between loss and validation values, where it's potentially performing slightly better in the training however it would suggest it would perform poorer those values where validated. The rsme value is still too high which again suggests the model will not perform accurate and could be off the predicted value of around 183 - 193."],"metadata":{"id":"92gfPOIoYMY5"}},{"cell_type":"code","source":["#new data\n","new_data_v2 = np.array([16,12,60,64,57,0.1,17,54,1027])\n","predictions = model_2014.predict(new_data_v2)\n","print(predictions)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KxsuY2s3Ya1R","executionInfo":{"status":"ok","timestamp":1683315426982,"user_tz":-60,"elapsed":447,"user":{"displayName":"Alanna Zimbehl","userId":"11428779735323257149"}},"outputId":"3da625b8-31bc-4559-ae4b-378f5c55ff37"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:6 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f82c67d0790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 51ms/step\n","[[393.49387 394.6226  394.9938  394.7438  395.607   395.08545 395.0645\n","  393.7133  394.00388 394.24844]]\n"]}]},{"cell_type":"markdown","source":["For this particular prediction test it takes in more values that the previous test. This one includes more features such as sea level pressure, wind speed, max and min temperatures. Based on similar data from the actual datasets and values, the model should predict the number of collisions between 555 and 720 which is quite a large range, however it is under performing based on the values passed in. Therefor like before despite changing the amount of data to a more specific time period and adding additional features to the model, it's still not accurate and performing rather poor."],"metadata":{"id":"jFefDMMfaR4Z"}},{"cell_type":"markdown","source":["##Deep Neural Networks"],"metadata":{"id":"yJBkaVAHBn7l"}},{"cell_type":"code","source":["#SCALE\n","SCALE = 1\n","\n","#scaling number of collisions - 2014\n","training_labels_2014 = training_labels_2014/SCALE\n","testing_labels_2014 = testing_labels_2014/SCALE\n","\n","#scaling number of collisions - full\n","training_labels_full = training_labels_full/SCALE\n","testing_labels_full = testing_labels_full/SCALE"],"metadata":{"id":"9yKRYWe3Br3V","executionInfo":{"status":"ok","timestamp":1683322100806,"user_tz":-60,"elapsed":392,"user":{"displayName":"Alanna Zimbehl","userId":"11428779735323257149"}}},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":["To better normalise some of the labels, it can be better to determine a scale and divide it by the scale value, to give the model a better chance of predicting accurately. In this case, each set of training and testing label will be divided by a global scale value which can then be used later to convert it back."],"metadata":{"id":"Q4CQaTnKf9Mj"}},{"cell_type":"code","source":["#normalising the values\n","normaliser_full = tf.keras.layers.Normalization(axis=-1)\n","normaliser_full.adapt(np.array(train_full_copy))\n","\n","#dnn model\n","dnn_model_1 = tf.keras.Sequential([\n","      normaliser_full,\n","      tf.keras.layers.Dense(48, activation='relu'),\n","      tf.keras.layers.Dense(32, activation='relu'),\n","      tf.keras.layers.Dense(16, activation='relu'),\n","      tf.keras.layers.Dense(8, activation='relu'),\n","      tf.keras.layers.Dense(1)\n","  ])\n","\n","dnn_model_1.compile(loss='mean_absolute_error',\n","                optimizer=tf.keras.optimizers.Adam(0.2),\n","                metrics = [tf.keras.metrics.RootMeanSquaredError(name='rmse')]\n","                )\n","     "],"metadata":{"id":"c__A6MtQDpTI","executionInfo":{"status":"ok","timestamp":1683322104116,"user_tz":-60,"elapsed":1254,"user":{"displayName":"Alanna Zimbehl","userId":"11428779735323257149"}}},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":["Like the regression model, it requires multiple steps to fine tune and build a model. They are similar, however difference being with a neural network there can be many layers of nodes that all connect to each other passing data once it passes a threshold. In this particular model, it was decided to scale down the nodes the deeper into the network until the final output layer where there would only be one node. Like before using the same loss functions such as RMSE to calculate the errors within the model between the predicted and actual values."],"metadata":{"id":"rAGQoGne1dff"}},{"cell_type":"code","source":["#build model\n","history = dnn_model_1.fit(\n","    train_full_copy,\n","    training_labels_full,\n","    validation_split=0.2,\n","    verbose=1, \n","    epochs=50)"],"metadata":{"id":"dV0479HlERnR","executionInfo":{"status":"ok","timestamp":1683322177315,"user_tz":-60,"elapsed":12292,"user":{"displayName":"Alanna Zimbehl","userId":"11428779735323257149"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"16a6d5b8-2814-491e-91a1-42f84b7495d8"},"execution_count":80,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","63/63 [==============================] - 0s 4ms/step - loss: 0.0124 - rmse: 0.0160 - val_loss: 0.0108 - val_rmse: 0.0141\n","Epoch 2/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0145 - rmse: 0.0186 - val_loss: 0.0124 - val_rmse: 0.0166\n","Epoch 3/50\n","63/63 [==============================] - 0s 4ms/step - loss: 0.0126 - rmse: 0.0163 - val_loss: 0.0104 - val_rmse: 0.0141\n","Epoch 4/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0165 - rmse: 0.0207 - val_loss: 0.0334 - val_rmse: 0.0359\n","Epoch 5/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0175 - rmse: 0.0220 - val_loss: 0.0105 - val_rmse: 0.0144\n","Epoch 6/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0135 - rmse: 0.0175 - val_loss: 0.0445 - val_rmse: 0.0466\n","Epoch 7/50\n","63/63 [==============================] - 0s 4ms/step - loss: 0.0194 - rmse: 0.0255 - val_loss: 0.0183 - val_rmse: 0.0208\n","Epoch 8/50\n","63/63 [==============================] - 0s 5ms/step - loss: 0.0133 - rmse: 0.0169 - val_loss: 0.0115 - val_rmse: 0.0145\n","Epoch 9/50\n","63/63 [==============================] - 0s 4ms/step - loss: 0.0133 - rmse: 0.0171 - val_loss: 0.0205 - val_rmse: 0.0244\n","Epoch 10/50\n","63/63 [==============================] - 0s 5ms/step - loss: 0.0172 - rmse: 0.0213 - val_loss: 0.0172 - val_rmse: 0.0197\n","Epoch 11/50\n","63/63 [==============================] - 0s 4ms/step - loss: 0.0193 - rmse: 0.0244 - val_loss: 0.0363 - val_rmse: 0.0388\n","Epoch 12/50\n","63/63 [==============================] - 0s 4ms/step - loss: 0.0278 - rmse: 0.0323 - val_loss: 0.0152 - val_rmse: 0.0178\n","Epoch 13/50\n","63/63 [==============================] - 0s 5ms/step - loss: 0.0135 - rmse: 0.0170 - val_loss: 0.0120 - val_rmse: 0.0162\n","Epoch 14/50\n","63/63 [==============================] - 0s 4ms/step - loss: 0.0134 - rmse: 0.0169 - val_loss: 0.0104 - val_rmse: 0.0140\n","Epoch 15/50\n","63/63 [==============================] - 0s 5ms/step - loss: 0.0164 - rmse: 0.0202 - val_loss: 0.0495 - val_rmse: 0.0514\n","Epoch 16/50\n","63/63 [==============================] - 0s 5ms/step - loss: 0.0180 - rmse: 0.0225 - val_loss: 0.0343 - val_rmse: 0.0370\n","Epoch 17/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0173 - rmse: 0.0221 - val_loss: 0.0154 - val_rmse: 0.0179\n","Epoch 18/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0124 - rmse: 0.0160 - val_loss: 0.0106 - val_rmse: 0.0140\n","Epoch 19/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0133 - rmse: 0.0168 - val_loss: 0.0104 - val_rmse: 0.0140\n","Epoch 20/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0118 - rmse: 0.0154 - val_loss: 0.0128 - val_rmse: 0.0155\n","Epoch 21/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0161 - rmse: 0.0198 - val_loss: 0.0105 - val_rmse: 0.0140\n","Epoch 22/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0126 - rmse: 0.0161 - val_loss: 0.0291 - val_rmse: 0.0321\n","Epoch 23/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0145 - rmse: 0.0183 - val_loss: 0.0110 - val_rmse: 0.0151\n","Epoch 24/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0158 - rmse: 0.0201 - val_loss: 0.0401 - val_rmse: 0.0425\n","Epoch 25/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0215 - rmse: 0.0266 - val_loss: 0.0104 - val_rmse: 0.0140\n","Epoch 26/50\n","63/63 [==============================] - 0s 4ms/step - loss: 0.0138 - rmse: 0.0177 - val_loss: 0.0104 - val_rmse: 0.0141\n","Epoch 27/50\n","63/63 [==============================] - 0s 4ms/step - loss: 0.0146 - rmse: 0.0186 - val_loss: 0.0148 - val_rmse: 0.0190\n","Epoch 28/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0163 - rmse: 0.0202 - val_loss: 0.0227 - val_rmse: 0.0253\n","Epoch 29/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0135 - rmse: 0.0172 - val_loss: 0.0215 - val_rmse: 0.0253\n","Epoch 30/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0180 - rmse: 0.0219 - val_loss: 0.0111 - val_rmse: 0.0152\n","Epoch 31/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0136 - rmse: 0.0174 - val_loss: 0.0180 - val_rmse: 0.0221\n","Epoch 32/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0151 - rmse: 0.0191 - val_loss: 0.0132 - val_rmse: 0.0175\n","Epoch 33/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0128 - rmse: 0.0165 - val_loss: 0.0156 - val_rmse: 0.0181\n","Epoch 34/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0140 - rmse: 0.0176 - val_loss: 0.0137 - val_rmse: 0.0179\n","Epoch 35/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0144 - rmse: 0.0183 - val_loss: 0.0130 - val_rmse: 0.0157\n","Epoch 36/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0120 - rmse: 0.0155 - val_loss: 0.0187 - val_rmse: 0.0212\n","Epoch 37/50\n","63/63 [==============================] - 0s 5ms/step - loss: 0.0211 - rmse: 0.0254 - val_loss: 0.0104 - val_rmse: 0.0141\n","Epoch 38/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0153 - rmse: 0.0192 - val_loss: 0.0215 - val_rmse: 0.0241\n","Epoch 39/50\n","63/63 [==============================] - 0s 6ms/step - loss: 0.0139 - rmse: 0.0175 - val_loss: 0.0218 - val_rmse: 0.0244\n","Epoch 40/50\n","63/63 [==============================] - 0s 7ms/step - loss: 0.0220 - rmse: 0.0268 - val_loss: 0.0104 - val_rmse: 0.0141\n","Epoch 41/50\n","63/63 [==============================] - 0s 5ms/step - loss: 0.0120 - rmse: 0.0156 - val_loss: 0.0108 - val_rmse: 0.0141\n","Epoch 42/50\n","63/63 [==============================] - 0s 5ms/step - loss: 0.0141 - rmse: 0.0180 - val_loss: 0.0106 - val_rmse: 0.0145\n","Epoch 43/50\n","63/63 [==============================] - 0s 4ms/step - loss: 0.0202 - rmse: 0.0248 - val_loss: 0.0106 - val_rmse: 0.0140\n","Epoch 44/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0141 - rmse: 0.0177 - val_loss: 0.0240 - val_rmse: 0.0275\n","Epoch 45/50\n","63/63 [==============================] - 0s 4ms/step - loss: 0.0200 - rmse: 0.0239 - val_loss: 0.0203 - val_rmse: 0.0228\n","Epoch 46/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0139 - rmse: 0.0177 - val_loss: 0.0104 - val_rmse: 0.0140\n","Epoch 47/50\n","63/63 [==============================] - 0s 5ms/step - loss: 0.0233 - rmse: 0.0280 - val_loss: 0.0192 - val_rmse: 0.0232\n","Epoch 48/50\n","63/63 [==============================] - 0s 6ms/step - loss: 0.0238 - rmse: 0.0281 - val_loss: 0.0337 - val_rmse: 0.0364\n","Epoch 49/50\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0183 - rmse: 0.0222 - val_loss: 0.0132 - val_rmse: 0.0160\n","Epoch 50/50\n","63/63 [==============================] - 0s 5ms/step - loss: 0.0154 - rmse: 0.0190 - val_loss: 0.0245 - val_rmse: 0.0279\n"]}]},{"cell_type":"markdown","source":["The summary of this model shows quite loss values however this may be due to the number of trips being scaled down. However with such low results it looks promosing. Again 80/20 split was decided for the validation method as it would allow the values to be checked, and in this instance there is a discrepency in some of the return results where the validated loss is almost double the training loss value. "],"metadata":{"id":"RfLbGeIa2SlI"}},{"cell_type":"code","source":["#predictions\n","dnn_model_1_results = dnn_model_1.evaluate(test_full_copy, testing_labels_full, verbose=1)\n","print(dnn_model_1_results)\n","\n","new_data_dnn = np.array([12,1,51,0.00,52])\n","predictions = dnn_model_1.predict(new_data_dnn)*SCALE\n","print(predictions)"],"metadata":{"id":"s3wcL4Z_FAr5","executionInfo":{"status":"ok","timestamp":1683323877292,"user_tz":-60,"elapsed":615,"user":{"displayName":"Alanna Zimbehl","userId":"11428779735323257149"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"02dca5fb-11fc-4392-a68c-77881644631e"},"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["20/20 [==============================] - 0s 2ms/step - loss: 0.0226 - rmse: 0.0260\n","[0.022640321403741837, 0.026029072701931]\n","1/1 [==============================] - 0s 28ms/step\n","[[0.07946528]]\n"]}]},{"cell_type":"markdown","source":["Loss results from the model are still low which should in theory mean that the accuracy of the dataset are good, however from the model's predictions it appears to still be inaccurate. The same inputs where passed into the model from the linear regression to determine if the neural network could be more accurate predictions. There could be amendments made to the dataset or the actual model itself perhaps using a fine tuning the optimiser or loss function."],"metadata":{"id":"LZ-TxzT72oQJ"}},{"cell_type":"code","source":["#2014 experiment\n","\n","#normalising the values\n","normaliser_full = tf.keras.layers.Normalization(axis=-1)\n","normaliser_full.adapt(np.array(train_2014_copy))\n","\n","#dnn model\n","dnn_model_2 = tf.keras.Sequential([\n","      normaliser_full,\n","      tf.keras.layers.Dense(48, activation='relu'),\n","      tf.keras.layers.Dense(24, activation='relu'),\n","      tf.keras.layers.Dense(1)\n","  ])\n","\n","dnn_model_2.compile(loss='mean_absolute_error',\n","                optimizer=tf.keras.optimizers.Adam(0.001))"],"metadata":{"id":"2J4AvLNIFu2x","executionInfo":{"status":"ok","timestamp":1683324153387,"user_tz":-60,"elapsed":1080,"user":{"displayName":"Alanna Zimbehl","userId":"11428779735323257149"}}},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":["Much like the previous model, it is made up of layers of nodes the difference being in this model is that there are less nodes and less layers. Furthermore the optimiser has been set to a lower rate of 0.001. "],"metadata":{"id":"SKevnlVZ2osO"}},{"cell_type":"code","source":["#build model\n","history = dnn_model_2.fit(\n","    train_2014_copy,\n","    training_labels_2014,\n","    validation_split=0.15,\n","    verbose=1, \n","    epochs=100)"],"metadata":{"id":"07-J58CeFu0r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683324164336,"user_tz":-60,"elapsed":7962,"user":{"displayName":"Alanna Zimbehl","userId":"11428779735323257149"}},"outputId":"d6da4e79-fb84-4882-da8a-4af06df18b3c"},"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","8/8 [==============================] - 1s 39ms/step - loss: 0.1937 - val_loss: 0.1125\n","Epoch 2/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.1261 - val_loss: 0.0881\n","Epoch 3/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.0793 - val_loss: 0.0732\n","Epoch 4/100\n","8/8 [==============================] - 0s 9ms/step - loss: 0.0652 - val_loss: 0.0676\n","Epoch 5/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0553 - val_loss: 0.0660\n","Epoch 6/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.0481 - val_loss: 0.0567\n","Epoch 7/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0409 - val_loss: 0.0547\n","Epoch 8/100\n","8/8 [==============================] - 0s 10ms/step - loss: 0.0354 - val_loss: 0.0493\n","Epoch 9/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0313 - val_loss: 0.0471\n","Epoch 10/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.0271 - val_loss: 0.0461\n","Epoch 11/100\n","8/8 [==============================] - 0s 7ms/step - loss: 0.0243 - val_loss: 0.0458\n","Epoch 12/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0221 - val_loss: 0.0428\n","Epoch 13/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0202 - val_loss: 0.0446\n","Epoch 14/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.0183 - val_loss: 0.0456\n","Epoch 15/100\n","8/8 [==============================] - 0s 9ms/step - loss: 0.0182 - val_loss: 0.0421\n","Epoch 16/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0174 - val_loss: 0.0416\n","Epoch 17/100\n","8/8 [==============================] - 0s 9ms/step - loss: 0.0172 - val_loss: 0.0395\n","Epoch 18/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.0176 - val_loss: 0.0420\n","Epoch 19/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0180 - val_loss: 0.0435\n","Epoch 20/100\n","8/8 [==============================] - 0s 9ms/step - loss: 0.0170 - val_loss: 0.0390\n","Epoch 21/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0158 - val_loss: 0.0393\n","Epoch 22/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.0139 - val_loss: 0.0367\n","Epoch 23/100\n","8/8 [==============================] - 0s 9ms/step - loss: 0.0141 - val_loss: 0.0381\n","Epoch 24/100\n","8/8 [==============================] - 0s 12ms/step - loss: 0.0138 - val_loss: 0.0355\n","Epoch 25/100\n","8/8 [==============================] - 0s 9ms/step - loss: 0.0129 - val_loss: 0.0369\n","Epoch 26/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0130 - val_loss: 0.0368\n","Epoch 27/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.0378\n","Epoch 28/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.0118 - val_loss: 0.0361\n","Epoch 29/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.0124 - val_loss: 0.0354\n","Epoch 30/100\n","8/8 [==============================] - 0s 9ms/step - loss: 0.0117 - val_loss: 0.0376\n","Epoch 31/100\n","8/8 [==============================] - 0s 9ms/step - loss: 0.0125 - val_loss: 0.0359\n","Epoch 32/100\n","8/8 [==============================] - 0s 9ms/step - loss: 0.0111 - val_loss: 0.0342\n","Epoch 33/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0352\n","Epoch 34/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.0106 - val_loss: 0.0352\n","Epoch 35/100\n","8/8 [==============================] - 0s 10ms/step - loss: 0.0109 - val_loss: 0.0372\n","Epoch 36/100\n","8/8 [==============================] - 0s 10ms/step - loss: 0.0105 - val_loss: 0.0340\n","Epoch 37/100\n","8/8 [==============================] - 0s 9ms/step - loss: 0.0112 - val_loss: 0.0352\n","Epoch 38/100\n","8/8 [==============================] - 0s 7ms/step - loss: 0.0109 - val_loss: 0.0342\n","Epoch 39/100\n","8/8 [==============================] - 0s 9ms/step - loss: 0.0100 - val_loss: 0.0334\n","Epoch 40/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0344\n","Epoch 41/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.0112 - val_loss: 0.0325\n","Epoch 42/100\n","8/8 [==============================] - 0s 7ms/step - loss: 0.0096 - val_loss: 0.0342\n","Epoch 43/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0323\n","Epoch 44/100\n","8/8 [==============================] - 0s 9ms/step - loss: 0.0096 - val_loss: 0.0346\n","Epoch 45/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0087 - val_loss: 0.0340\n","Epoch 46/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0090 - val_loss: 0.0331\n","Epoch 47/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0086 - val_loss: 0.0325\n","Epoch 48/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0085 - val_loss: 0.0330\n","Epoch 49/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0090 - val_loss: 0.0331\n","Epoch 50/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.0086 - val_loss: 0.0332\n","Epoch 51/100\n","8/8 [==============================] - 0s 9ms/step - loss: 0.0081 - val_loss: 0.0310\n","Epoch 52/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0083 - val_loss: 0.0311\n","Epoch 53/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0307\n","Epoch 54/100\n","8/8 [==============================] - 0s 10ms/step - loss: 0.0105 - val_loss: 0.0319\n","Epoch 55/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0339\n","Epoch 56/100\n","8/8 [==============================] - 0s 9ms/step - loss: 0.0131 - val_loss: 0.0340\n","Epoch 57/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.0122 - val_loss: 0.0312\n","Epoch 58/100\n","8/8 [==============================] - 0s 9ms/step - loss: 0.0096 - val_loss: 0.0305\n","Epoch 59/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0086 - val_loss: 0.0318\n","Epoch 60/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.0090 - val_loss: 0.0305\n","Epoch 61/100\n","8/8 [==============================] - 0s 7ms/step - loss: 0.0081 - val_loss: 0.0292\n","Epoch 62/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0087 - val_loss: 0.0306\n","Epoch 63/100\n","8/8 [==============================] - 0s 9ms/step - loss: 0.0076 - val_loss: 0.0307\n","Epoch 64/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.0079 - val_loss: 0.0306\n","Epoch 65/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0086 - val_loss: 0.0281\n","Epoch 66/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0077 - val_loss: 0.0316\n","Epoch 67/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0091 - val_loss: 0.0301\n","Epoch 68/100\n","8/8 [==============================] - 0s 10ms/step - loss: 0.0096 - val_loss: 0.0288\n","Epoch 69/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.0079 - val_loss: 0.0280\n","Epoch 70/100\n","8/8 [==============================] - 0s 9ms/step - loss: 0.0076 - val_loss: 0.0276\n","Epoch 71/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.0071 - val_loss: 0.0282\n","Epoch 72/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0072 - val_loss: 0.0307\n","Epoch 73/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0068 - val_loss: 0.0287\n","Epoch 74/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0057 - val_loss: 0.0275\n","Epoch 75/100\n","8/8 [==============================] - 0s 9ms/step - loss: 0.0067 - val_loss: 0.0285\n","Epoch 76/100\n","8/8 [==============================] - 0s 9ms/step - loss: 0.0069 - val_loss: 0.0279\n","Epoch 77/100\n","8/8 [==============================] - 0s 9ms/step - loss: 0.0068 - val_loss: 0.0283\n","Epoch 78/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0071 - val_loss: 0.0279\n","Epoch 79/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0077 - val_loss: 0.0294\n","Epoch 80/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0286\n","Epoch 81/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0075 - val_loss: 0.0280\n","Epoch 82/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0083 - val_loss: 0.0281\n","Epoch 83/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.0080 - val_loss: 0.0275\n","Epoch 84/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0271\n","Epoch 85/100\n","8/8 [==============================] - 0s 9ms/step - loss: 0.0073 - val_loss: 0.0274\n","Epoch 86/100\n","8/8 [==============================] - 0s 7ms/step - loss: 0.0060 - val_loss: 0.0264\n","Epoch 87/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0064 - val_loss: 0.0263\n","Epoch 88/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.0066 - val_loss: 0.0270\n","Epoch 89/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0067 - val_loss: 0.0250\n","Epoch 90/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0063 - val_loss: 0.0255\n","Epoch 91/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0088 - val_loss: 0.0254\n","Epoch 92/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.0098 - val_loss: 0.0257\n","Epoch 93/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0253\n","Epoch 94/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.0086 - val_loss: 0.0296\n","Epoch 95/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.0091 - val_loss: 0.0248\n","Epoch 96/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0068 - val_loss: 0.0263\n","Epoch 97/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.0066 - val_loss: 0.0248\n","Epoch 98/100\n","8/8 [==============================] - 0s 6ms/step - loss: 0.0063 - val_loss: 0.0264\n","Epoch 99/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0266\n","Epoch 100/100\n","8/8 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0258\n"]}]},{"cell_type":"markdown","source":["In this model there have been a few slight changes to the setup for example, since the data inputs is much smaller as it's using the 2014 dataset, the validation split was reduced to 0.15 which is 15%. The epochs where increased to 100 which will allow for the dataset to be passed through more. Again the loss results are relatively low however the validation loss is much higher which would suggest that the during the validation stage there where either incorrect estimates or the model won't make accurate predictions during testing."],"metadata":{"id":"a4F4IdGg2pI1"}},{"cell_type":"code","source":["#predictions\n","dnn_model_2_results = dnn_model_2.evaluate(test_2014_copy, testing_labels_2014, verbose=0)\n","print(dnn_model_2_results)\n","\n","#new data\n","new_data_dnn_v2 = np.array([16,12,60,64,57,0.1,17,54,1027])\n","predictions = dnn_model_2.predict(new_data_dnn_v2) * SCALE\n","print(predictions)"],"metadata":{"id":"CGVYa15CFuyv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683324293209,"user_tz":-60,"elapsed":359,"user":{"displayName":"Alanna Zimbehl","userId":"11428779735323257149"}},"outputId":"15bf4558-66e5-4359-f991-d42959ffac0e"},"execution_count":86,"outputs":[{"output_type":"stream","name":"stdout","text":["0.027196988463401794\n","1/1 [==============================] - 0s 64ms/step\n","[[0.04488176]]\n"]}]},{"cell_type":"markdown","source":["The model was passed in the same new dataset from the 2014 linear regression model to find out if it could make more accurate predictions. While the loss result is still relatively low the prediction value again is inaccurate. At this stage it may be that the data being used don't have a strong enough corelation to the number of collisions to make accurate enough predictions."],"metadata":{"id":"bRriTTty2pmo"}},{"cell_type":"markdown","source":["#Conclusion\n","\n","In conclusion based on the results from both the neural network and the linear regression, it can be concluded there while there are definite trends between the weather and the number of collisions it doesn't appear to be enough to make accurate predictions. During the analysis stage there was slight corelations between weather features and the number of collisions however it's highly likely there are a number of other factors that could influence the number of collisions. It was observed especially in the year 2020 that there was noticeable spikes in the number of collision throughout the months until march, where it then drastically dropped due to covid which meant that the amount of people traveling had significantly dropped. Throughout the other years there where noticeable spikes during the holiday seasons again suggests traffic increases and could therefor affect the number of collisions. This combined with varying weather could infact effect the number of collisions however currently, based on the models it's apparent it may not be strong enough to make accurate enough predictions where emergency services are able to efficiently allocate resources. There could be further improvements to models, such as having more available data perhaps from bigger datasets, as well as further data cleansing on the original dataset which could therefor allow the model to make more accurate predictions. \n"],"metadata":{"id":"9OiMypuUQyPo"}},{"cell_type":"markdown","source":["#Resources  \n","1. [Keras Loss Functions](https://neptune.ai/blog/keras-loss-functions)\n","\n","#Reference List\n","\n","- IBM. (2023). *What are neural networks?* [Online]. Available at: https://www.ibm.com/topics/neural-networks \n","[Accessed on 4th May 2023]\n","\n","- IBM. (2023). *Linear regression* [Online]. Available at: https://www.ibm.com/topics/linear-regression\n","[Accessed on 4th May 2023]\n","\n","- Neptune Ai. (2023). *Keras Loss Functions* [Online]. Available at: https://neptune.ai/blog/keras-loss-functions\n","[Accessed on 4th May 2023]"],"metadata":{"id":"KaVZVNe8-DhV"}}]}